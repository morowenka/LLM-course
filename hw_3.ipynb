{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_MmJIQqJh9tp"},"outputs":[],"source":["#!pip install datasets sentence-transformers scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"RtYfds2_h9tr"},"source":["# Task: Text Clustering with Custom KMeans"]},{"cell_type":"markdown","metadata":{"id":"q-p_DVA-h9ts"},"source":["Using sentence_transformers, convert a set of texts into embeddings, and apply clustering using a custom implementation of KMeans. The goal is to group similar texts based on different distance metrics (such as cosine similarity, Euclidean distance, or Manhattan distance) implemented in the KMeans algorithm."]},{"cell_type":"markdown","metadata":{"id":"TrhPuZ6Sh9tt"},"source":["## 1. Data loading"]},{"cell_type":"markdown","metadata":{"id":"FgKvIsfeh9tt"},"source":["Load the GO Emotions dataset from Hugging Face to obtain the texts that will be clustered. If you prefer some other dataset you can use it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wt0iQX3uh9tt"},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset('google-research-datasets/go_emotions', split='train')\n"]},{"cell_type":"markdown","metadata":{"id":"EsmoAPIhh9tu"},"source":["Print some examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p6E9hABAh9tu"},"outputs":[],"source":["for i in range(5):\n","    print(f\"Text {i+1}: {dataset[i]['text']}\")"]},{"cell_type":"markdown","metadata":{"id":"PYBZYOhrh9tu"},"source":["## 2. Convert texts to embeddings"]},{"cell_type":"markdown","metadata":{"id":"DJ8jabSbh9tu"},"source":["Use the sentence-transformers library to transform the texts into embeddings (numerical vectors) for clustering."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1W3aEGIh9tv"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","import numpy as np\n","\n","#TODO\n","\n","print(f\"Shape of embeddings: {embeddings.shape}\")\n"]},{"cell_type":"markdown","metadata":{"id":"lx5tfv4Wh9tv"},"source":["## 3. Implement custom KMeans"]},{"cell_type":"markdown","metadata":{"id":"zOmBAK2nh9tv"},"source":["Create a custom implementation of KMeans, supporting at leas four different distance metrics (Correlation, Euclidean, Cosine, Mahalanobis). You shouldn't use any specific libs, only numpy.\n","Here are the formulas for correlation distance and Mahalanobis distance:\n","\n","#### Correlation Distance\n","Correlation distance measures the difference in the direction of vectors rather than their magnitude. The formula for correlation distance is:\n","\n","$$\n","d_{\\text{corr}}(a, b) = 1 - \\frac{\\sum_{i=1}^{n}(a_i - \\bar{a})(b_i - \\bar{b})}{\\sqrt{\\sum_{i=1}^{n}(a_i - \\bar{a})^2} \\sqrt{\\sum_{i=1}^{n}(b_i - \\bar{b})^2}}\n","$$\n","\n","where:\n","- $a$ and $b$ are two vectors,\n","- $\\bar{a}$ and $\\bar{b}$ are the mean values of the components of vectors $a$ and $b$, respectively.\n","\n","or using ```np.corrcoef```:\n","\n","$$\n","d_{\\text{corr}}(a, b) = 1 - np.corrcoef(a, b)[0,1]\n","$$\n","\n","\n","#### Mahalanobis Distance\n","Mahalanobis distance accounts for not only the distance between points but also the covariance between them. The formula is:\n","\n","$$\n","d_{\\text{mahal}}(a, b) = \\sqrt{(a - b)^T S^{-1} (a - b)}\n","$$\n","\n","where:\n","- $a$ and $b$ are two vectors,\n","- $S$ is the covariance matrix of the features,\n","- $S^{-1}$ is the inverse covariance matrix.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlEb9T12h9tv"},"outputs":[],"source":["import numpy as np\n","\n","def euclidean_distance(a, b):\n","    #TODO\n","\n","def cosine_distance(a, b):\n","    #TODO\n","\n","def mahalanobis_distance(a, b, **kwargs):\n","    #TODO\n","\n","def correlation_distance(a, b):\n","    #TODO"]},{"cell_type":"markdown","metadata":{"id":"A0ntLar8h9tv"},"source":["Implement custom K-Means class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJYjA3R8h9tv"},"outputs":[],"source":["class CustomKMeans:\n","    def __init__(self, n_clusters=3, max_iter=100, distance_metric='euclidean',  **kwargs):\n","        self.n_clusters = n_clusters\n","        self.max_iter = max_iter\n","        self.distance_metric = distance_metric\n","        self.centroids = None\n","        self.labels_ = None\n","        self.kwargs = kwargs # inverted covariation matrix for mahalanodis disntace\n","\n","    def fit(self, X):\n","        random_indices = np.random.choice(len(X), self.n_clusters, replace=False)\n","        self.centroids = X[random_indices]\n","\n","        for _ in range(self.max_iter):\n","            self.labels_ = np.array([self._assign_label(x) for x in X])\n","\n","            new_centroids = np.array([X[self.labels_ == i].mean(axis=0) if len(X[self.labels_ == i]) > 0 else self.centroids[i] for i in range(self.n_clusters)])\n","\n","            if np.all(self.centroids == new_centroids):\n","                break\n","            self.centroids = new_centroids\n","\n","    def _assign_label(self, x):\n","        # assigns each data point to the nearest centroid based on the chosen distance metric.\n","        # method returns the index of the closest centroid, which represents the cluster assignment (or label) for the given data point x\n","        #TODO\n","        return np.argmin(distances)\n"]},{"cell_type":"markdown","metadata":{"id":"mWQk5rplh9tv"},"source":["## 4. Fit the custom K-Means"]},{"cell_type":"markdown","metadata":{"id":"68KYzIp_h9tw"},"source":["Fit the custom KMeans model to the dataset using all of the distance metrics and obtain cluster labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5-eOPEy5h9tw"},"outputs":[],"source":["n_clusters = 10\n","#cov_matrix = np.cov(embeddings, rowvar=False)\n","#inv_cov_matrix = np.linalg.inv(cov_matrix)\n","\n","custom_kmeans = CustomKMeans(n_clusters=10, distance_metric='correlation')\n","custom_kmeans.fit(embeddings)\n","\n","labels = custom_kmeans.labels_\n","\n","print(\"Custom KMeans labels for the first 10 texts:\")\n","print(labels[:10])\n"]},{"cell_type":"markdown","metadata":{"id":"JLBGzuBfh9tw"},"source":["## 5. Visualize the results"]},{"cell_type":"markdown","metadata":{"id":"S9igxv6Gh9tw"},"source":["Visualize the clusters by reducing the dimensionality of the embeddings using PCA and plotting the clusters in 2D space.  \n","You should get something like this:\n","\n","<a href=\"https://ibb.co/nRY9hQf\"><img src=\"https://i.ibb.co/zNBpKPb/output.png\" alt=\"output\" border=\"0\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-2IQVMjh9tw"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","\n","#TODO"]},{"cell_type":"markdown","metadata":{"id":"E1sXLNhsh9tw"},"source":["Let's print examples of the text for each cluster"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qf3YGr0Sh9tw"},"outputs":[],"source":["for cluster in range(n_clusters):\n","    print(f\"\\nCluster {cluster}:\")\n","    cluster_texts = [texts[i] for i in range(len(texts)) if labels[i] == cluster]\n","    for text in cluster_texts[:5]:\n","        print(f\"- {text}\")"]},{"cell_type":"markdown","metadata":{"id":"dxZVtxPCh9tw"},"source":["## 6. Report"]},{"cell_type":"markdown","metadata":{"id":"sMgKjdYZh9tw"},"source":["Make a conclusion and write a short report. What are the differnes between the methods used? What are their limitations? What is the applicability of each?"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"}},"nbformat":4,"nbformat_minor":0}
